{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22df8260",
   "metadata": {},
   "source": [
    "![imagem](images/VLL_Banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f266c1",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 24px; text-align: center;font-weight: bold\">ANÁLISE DAS CONTRATAÇÕES NO ESTADO DE SANTA CATARINA:</div><br>\n",
    "\n",
    "<div style=\"font-size: 20px; text-align: center\">JPNB 01 - ETL DOS DADOS DE FORNECEDORES</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168ec6df",
   "metadata": {},
   "source": [
    "<span style='font-size: 14px'>Autor: [Maurício Vasconcellos Leão Lyrio, Dr.](https://br.linkedin.com/in/maur%C3%ADcio-vasconcellos-le%C3%A3o-lyrio-59773220) | Página Oficial: www.vll.adm.br</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825ed750",
   "metadata": {},
   "source": [
    "---\n",
    "# Instalação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5326417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulação de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Interação com bancos de dados\n",
    "# SQLite\n",
    "import sqlite3\n",
    "# MySQL\n",
    "#import mysql.connector\n",
    "# MongoDB\n",
    "#from pymongo import MongoClient\n",
    "\n",
    "# Visualização de dados\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "# Ignorar warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Versões dos pacotes utilizados neste Jupyter notebook\n",
    "#!pip install -q -U watermark\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Mauricio Vasconcellos Leão Lyrio | vll.adm.br\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532c988c",
   "metadata": {},
   "source": [
    "---\n",
    "# Carregamento dos datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ae7d2a",
   "metadata": {},
   "source": [
    "Para iniciar nosso processo de análise de dados iremos **carregar o dataset com os dados cadastrais de fornecedores** recebido por meio de solicitação via LAI. Esse procedimento carrega o dataset do arquivo .csv original e o armazena em um dataframe pandas denominado ***df***, para que possamos manipulá-lo posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee4f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o dataset de fornecedores\n",
    "df = pd.read_csv('datasets/ELIC_fornecedores_cadastro.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da184819",
   "metadata": {},
   "source": [
    "---\n",
    "# Análise exploratória dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aec91d",
   "metadata": {},
   "source": [
    "Após carregar o dataset damos início ao processo de análise exploratório, buscando **analisar a qualidade e integridade dos dados**. O processo de análise exploratório nos ajuda a ter uma visão geral do dataset e que tipo de pré-processamento precisaremos realizar nos dados a fim de deixá-los prontos para as etapas posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1705407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando se o dataset foi carregado corretamente e seu tipo\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f2df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando o formato do dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fbe946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listando as colunas do dataframe\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3244b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listando as informações gerais do dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783254c7",
   "metadata": {},
   "source": [
    "Até o momento conseguimos carregar os dados, verificar o tamanho do dataframe, suas colunas, o tipo de dado de cada coluna e se existem valores ausentes. Pelo info do dataframe é possível perceber que existem campos com valores nulos. Precisaremos definir o que fazer com esses campos e que tipo de tratamento iremos dar para os valores nulos, voltaremos a discutir essa questão na fase do pré-processamento de dados.\n",
    "\n",
    "Pelo info do dataframe também é possível perceber que quase todos os campos são não-numéricos. O único campo numérico é o ***cnpj***, que, na verdade, não é uma variável quantitativa e sim um código que representa o cadastro de pessoa jurídica do fornecedor. Posteriormente iremos ajustar o tipo de dado desse campo, por hora iremos somente descrever os demais campos de nosso dataset e visualizar uma amostra dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea0c5b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Descrevendo os dados não-numéricos\n",
    "df.describe(include=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9555f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51cc9f6",
   "metadata": {},
   "source": [
    "Com a visualização de uma amostra do dataset finalizamos a análise exploratória. Outros tipos de análise poderiam ser feitos nessa fase, porém, para nosso objetivo de preparar o dataset o que vimos até agora é suficiente. Passemos então à próxima fase do processo, o pré-processamento dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d56af5",
   "metadata": {},
   "source": [
    "---\n",
    "# Pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065ea1f0",
   "metadata": {},
   "source": [
    "Na fase de análise exploratória identificamos que nosso dataset possui campos nulos e também que um dos campos foi definido de forma equivocada como numérico. Vamos agora tratar esses problemas e também analisar a necessidade de outros tipos de transformação de dados. Comecemos com a limpeza dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a88ef1",
   "metadata": {},
   "source": [
    "## Limpeza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb16944",
   "metadata": {},
   "source": [
    "Conforme visto anteriormente, nosso dataset possui uma série de campos com valores nulos. Vamos analisar melhor essa situação e definir o que fazer com esses valores. para isso criaremos uma nova **tabela com a distribuição percentual de valores nulos por coluna**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d87929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma lista vazia para armazenar as informações de nome e tipo de coluna.\n",
    "colunas_info = []\n",
    "\n",
    "# Iterando pelas colunas do dataframe\n",
    "for coluna in df.columns:\n",
    "    coluna_nome = coluna\n",
    "    coluna_tipo = df[coluna].dtype\n",
    "    coluna_nulos = df[coluna].isnull().sum()\n",
    "    coluna_nulos_perc = (coluna_nulos/len(df))*100\n",
    "    colunas_info.append((coluna_nome,coluna_tipo,coluna_nulos,coluna_nulos_perc))\n",
    "                        \n",
    "# Criando um novo dataframe e exibindo as informações das colunas\n",
    "df_colunas_info = pd.DataFrame(colunas_info, columns=['Coluna','Tipo','Q Nulo', '% Nulo'])\n",
    "print(df_colunas_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690586b4",
   "metadata": {},
   "source": [
    "Com a nova tabela fica mais fácil evidenciar os valores ausentes do dataframe. No caso, as colunas ***cidade***, ***uf***, ***pais*** e ***produtos_habilitados*** apresentam valores ausentes. Em projetos de datascience, em geral, utiliza-se como regra para tratamento de valores ausentes as seguintes opções:\n",
    "\n",
    "- Para valores ausentes >= 50%, descartamos a variável;\n",
    "- Para valores ausentes < 50%, tratar os valores ausentes;\n",
    "- Para valores ausentes < 2%, descartar os valores ausentes.\n",
    "\n",
    "Apesar dessa regra geral, é importante analisar o dataframe e verificar a forma mais adequada para tratamento dos valores ausentes e, principalmente, justificar as escolhas feitas no decorrer do tratamento dos dados. Em nosso caso, **como os valores ausentes são menos de 2% dos valores dos campos vamos excluí-los**. Porém, vale salientar que, ao excluir os registros cujos campos estão ausentes perdemos parte da informação no dataset, vale refletir sobre a relevância da perda dessa informação.\n",
    "\n",
    "Nesse caso, como os registros no dataset estão com granularidade definida em nível de produtos habilitados, acredita-se que os dados básicos dos fornecedores não serão perdidos devido à exclusão desses registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d0938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma cópia do dataframe original e excluindo os registros com valores nulos.\n",
    "df1 = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd9f720",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4e9dea",
   "metadata": {},
   "source": [
    "Temos agora um novo dataframe ***df1*** com os registros com valore ausentes excluídos. Mantivemos em memória o dataset original para o caso de queremos retornar à essa versão em outro momento. Passemos agora à tranformação dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9bd8b6",
   "metadata": {},
   "source": [
    "## Transformação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eb4a49",
   "metadata": {},
   "source": [
    "Iniciaremos nossa fase de transformação de dados ajustando o tipo de dado referente à coluna ***cnpj*** do fornecedor. Conforme verificamos anteriormente, essa coluna foi definida como numérica, do tipo int64. Porém, por se tratar de um código de referência de cada fornecedor, ela na verdade é categórica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c3f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alterando o tipo de dado da coluna cnpj\n",
    "df1['cnpj'] = df1['cnpj'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe7320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listando as informações gerais para conferir a alteração\n",
    "df1['cnpj'].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2759083",
   "metadata": {},
   "source": [
    "Conforme evidenciado acima, agora todos os campos de nosso dataset são campos categóricos e poderemos utilizar os métodos de string para a coluna. Dando sequência á transformação de dados, analisando a coluna ***produtos_habilitados*** percebemos que essa coluna apresenta a informação dos produtos concatenando duas informações, o grupo e a classe do produto.\n",
    "\n",
    "Esse tipo de conhecimento é o que chamamos de conhecimento de negócio, ou seja, para perceber esses nuances nos dados o analista precisa conhecer pelo menos um pouco da área de negócio sobre a qual está trabalhando, dessa forma a possibilidade de ter insights úteis em relação ao tópico aumenta, aprimorando as possibilidades de análise.\n",
    "\n",
    "Iremos separar a coluna de ***produtos_habilitados*** em duas colunas, a primeira para armazenar os grupos de produtos e a segunda para armazenar a classe dos produtos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16342e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando as novas colunas e separando os registros da coluna original\n",
    "df1[['Grupo','Classe','Descrição']] = df1['produtos_habilitados'].str.split(' - ',2,expand=True)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3290f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo a coluna ***Grupo*** em duas para separar os códigos de grupo e de classe.\n",
    "df1['Grupo1'] = df1['Grupo'].str[0:2]\n",
    "df1['Grupo2'] = df1['Grupo'].str[2:]\n",
    "\n",
    "# Renomeando a coluna de grupo/classe original\n",
    "df1.rename(columns={'Grupo':'Código GC'},inplace=True)\n",
    "\n",
    "# Criando as colunas de descrição de grupo e de classe\n",
    "df1['Grupo_desc'] = df1['Grupo1']+' - '+df1['Classe']\n",
    "df1['Classe_desc'] = df1['Grupo2']+' - '+df1['Descrição']\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ea0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluindo as colunas que não serão necessárias\n",
    "df1.drop(columns=['Classe','Descrição','Grupo1','Grupo2'],inplace=True)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1a2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustando os nomes das novas colunas\n",
    "df1.rename(columns={'Grupo_desc':'Grupo','Classe_desc':'Classe'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78cdcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0c4b0b",
   "metadata": {},
   "source": [
    "Ao gerar o info do novo dataset percebemos que ao transformar o dataset original acabamos por gerar uma nova coluna que contém valores nulos. Vamos agora analisar a situação e decidir o que fazer com os dados inconsistentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb345ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descrevendo os dados não-numéricos\n",
    "df1.describe(include=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d99cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparando a descrição dos dataframes\n",
    "describe_df = df.describe(include=object)\n",
    "describe_df1 = df1.describe(include=object)\n",
    "\n",
    "# Adicionando os nomes dos dataframes como índices\n",
    "describe_df.index=['df'] * len(describe_df)\n",
    "describe_df1.index=['df1'] * len(describe_df)\n",
    "\n",
    "# Concatenando os resultados em um único dataframe\n",
    "df_comparacao = pd.concat([describe_df,describe_df1])\n",
    "\n",
    "# Listando a tabela resultante\n",
    "df_comparacao"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a1a9e4",
   "metadata": {},
   "source": [
    "Comparando a descrição do ***df1*** com a descrição do ***df*** percebemos que no processo de limpeza e transformação de dados foram perdidos registros de fornecedores (o df original tinha 20.121 cnpjs únicos e o novo df1 possui 19.194). Conforme dito anteriormente, a decisão de manter ou não determinadas informações do dataset é do analista, porém, é importante ter em conta as justificativas para as escolhas feitas no decorrer do processo. No caso, optaremos por manter o dataframe com os registros de *Classe* nulos, dado que já havíamos perdido informações de 927 fornecedores na primeira etapa do processo de limpeza.\n",
    "\n",
    "Porém, para que o dataframe se torne mais \"amigável\", vamos substituir os campos nos quais existem valores nulos por um valor categórico para fins de uso na etapa de análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f659c6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preenchendo valores nulos da categoria Classe\n",
    "df1['Classe'].fillna('Classe não definida',inplace=True)\n",
    "\n",
    "# Verificando o resultado\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c54a68b",
   "metadata": {},
   "source": [
    "Com a finalização do processo de tranformação de dados nosso dataframe está pronto para ser carregado em banco de dados ou exportado em formatos de arquivos para análise posterior. É o que iremos fazer agora, na fazer de geração de dados de saída."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419b387f",
   "metadata": {},
   "source": [
    "---\n",
    "# Geração de dados de saída (Data output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7985d7d",
   "metadata": {},
   "source": [
    "Uma vez finalizada a fase de limpeza e transformação, agora iremos dar saída ao dataset gerado para fins de análise. Faremos isso em forma de arquivos e em registros em banco de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88de4387",
   "metadata": {},
   "source": [
    "## Gravação em arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4784b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gravando em formato .csv\n",
    "df1.to_csv('datasets/df1_fornecedores.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9f51c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gravando em formato .json\n",
    "df1.to_json('datasets/df1_fornecedores.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d9d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gravando em formato .xls\n",
    "df1.to_excel('datasets/df1_fornecedores.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f1005b",
   "metadata": {},
   "source": [
    "## Gravação em DB relacional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cd2f4e",
   "metadata": {},
   "source": [
    "> Gravando em banco de dados SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c506b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando a biblioteca para interação com o SQLite\n",
    "#import sqlite3\n",
    "\n",
    "# Criando uma conexão ao banco de dados SQLite\n",
    "cnn=sqlite3.connect('database/da12023db.db')\n",
    "\n",
    "# Copiando nosso dataframe para o banco de dados\n",
    "df1.to_sql('Fornecedores',cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13530a33",
   "metadata": {},
   "source": [
    "> Abaixo o código para gravação em banco de dados ***MySQL***, não executaremos esse script porque necessitamos ter  SGBD instalado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bddc171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando a biblioteca para interação com o MySQL\n",
    "#import mysql.connector\n",
    "\n",
    "# Configurando os parâmetros da conexão\n",
    "#config={\n",
    "#    'user':'seu usuário';\n",
    "#    'password':'sua_senha';\n",
    "#    'host':'localhost';    # ou endereço do servidor MySQL\n",
    "#    'database':'seu_bando_de_dados'\n",
    "#}\n",
    "\n",
    "# Criando uma conexão ao banco de dados\n",
    "#try:\n",
    "#    conn=mysql.connector.connect(**config)\n",
    "#    if conn.is_connected():\n",
    "#        print('Conexão ao banco de dados bem sucedida')\n",
    "#except mysql.connector.Error as err:\n",
    "#    print(f'Erro ao conectar ao banco de dados: {err}')\n",
    "\n",
    "# Copiando nosso dataframe para o banco de dados\n",
    "\n",
    "# Fechando a conexão (ao terminar de utilizar)\n",
    "#conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5511e5c7",
   "metadata": {},
   "source": [
    "## Gravação em datalake (BD não-relacional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bd7faf",
   "metadata": {},
   "source": [
    "> Abaixo o código para gravação em banco de dados ***MongoDB***, não executaremos esse script porque necessitamos ter  SGBD instalado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c0763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando a biblioteca para interação com o MongoDB\n",
    "#from pymongo import MongoClient\n",
    "\n",
    "# Configurando os parâmetros de conexão\n",
    "#client=MongoClient('mongodb://localhost:27017/')\n",
    "\n",
    "# Acessando um banco de dados específico\n",
    "#db=client['appdb']\n",
    "\n",
    "# Criando uma coleção no banco de dados\n",
    "#collection = db['Fornecedores']\n",
    "\n",
    "# Carregando o dataframe\n",
    "#data = df1.to_dict(orient='records')\n",
    "\n",
    "# Inserindo os registros no MongoDB\n",
    "#collection.insert_many(data)\n",
    "\n",
    "# Listando as coleções disponíveis\n",
    "#print(db.list_collection_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd652cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fornecedores=collection.find()\n",
    "#for fornecedor in fornecedores:\n",
    "#    print(fornecedor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d7eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fechando a conexão ao MongoDB\n",
    "#client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649eae84",
   "metadata": {},
   "source": [
    "Com a geração dos dados de saída, nosso trabalho de ***ETL*** de dados terminou. Fizemos a limpeza, transformação e carga de dados para arquivos de saíde e bancos de dados relacionais e não-relacionais. Agora passaremos à etapa de visualização de dados (dataviz) que faremos utilizando o Microsoft Power BI como ferramenta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dedbc5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9071f326",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 32px; text-align: center;font-weight: bold\">FIM</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa4660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versão da linguagem Python e arquitetura do Jupyter Notebook\n",
    "import platform\n",
    "print('Versão da linguagem Python utilizada neste notebook:', platform.python_version())\n",
    "print('Arquitetura do Jupyter utilizada neste notebook:', platform.architecture()[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
